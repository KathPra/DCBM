{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import wandb\n",
    "import re\n",
    "import clip\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    jaccard_score,\n",
    "    normalized_mutual_info_score,\n",
    ")\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# add the parent directory to the path\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "                \n",
    "from utils.dcbm import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data & Apply Concept extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Fix -----------------\n",
    "embed_path = \"../data/Embeddings/\"\n",
    "dataset = \"cub\"\n",
    "class_labels_path = \"../data/classes/cub_classes.txt\"\n",
    "segment_path = \"../data/Segments/\"\n",
    "selected_image_concepts = \"../data/Embeddings/subsets\"\n",
    "\n",
    "# ----------------- Hyperparameters -----------------\n",
    "\n",
    "model_name = \"CLIP-ViT-L14\"  # \"CLIP-ViT-L14\", \"CLIP-RN50\", CLIP-ViT-B16\n",
    "\n",
    "segmentation_technique = \"SAM2\"  # GDINO, SAM, SAM2, DETR, MaskRCNN\n",
    "concept_name = None # Define for GDINE [awa, sun, sun-lowthresh, cub...]\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "cluster_array = [128, 256, 512, 1024, 2048]\n",
    "cluster_method = \"kmeans\"  # \"hierarchical\", \"kmeans\"\n",
    "centroid_method = \"median\"  # \"mean\", \"median\"\n",
    "\n",
    "concept_per_class = 50  # How many images for each class 5,10,20,50, None\n",
    "\n",
    "one_hot = False\n",
    "epochs = 200\n",
    "lambda_1 = 1e-4\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "\n",
    "crop = False  # True without background\n",
    "\n",
    "use_wandb = False\n",
    "project = \"YOUR_PROJECT_NAME\"  # Define your own project name within wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segcrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_array_1 = {}\n",
    "\n",
    "for clusters in cluster_array:\n",
    "    cluster_array_1[clusters] = []\n",
    "    model_names = [\"CLIP-ViT-B16\", \"CLIP-ViT-L14\", \"CLIP-RN50\"]\n",
    "\n",
    "    for model_name in model_names:\n",
    "        cbm = CBM(embed_path, dataset, model_name, class_labels_path)\n",
    "        cbm.load_concepts(\n",
    "            segment_path,\n",
    "            segmentation_technique,\n",
    "            concept_name,\n",
    "            selected_image_concepts,\n",
    "            concept_per_class,\n",
    "        )\n",
    "\n",
    "        norm = np.linalg.norm(cbm.image_segments, axis=1, keepdims=True)\n",
    "        image_embedding = cbm.image_segments / norm\n",
    "        # image_embedding = cbm.image_segments\n",
    "\n",
    "        if clusters > image_embedding.shape[0]:\n",
    "            clusters = image_embedding.shape[0]\n",
    "\n",
    "        print(\"Amount of image embeddings: \", image_embedding.shape[0])\n",
    "        print(\"Amount of clusters: \", clusters)\n",
    "\n",
    "        pca = PCA(n_components=100)  # You can adjust the number of components as needed\n",
    "        image_embedding_pca = pca.fit_transform(image_embedding)\n",
    "\n",
    "        cbm.cluster_technique = cluster_method\n",
    "        if cluster_method == \"kmeans\":\n",
    "            clustering_model = KMeans(n_clusters=clusters, random_state=42)\n",
    "            cluster_ids = clustering_model.fit_predict(image_embedding_pca)\n",
    "        elif cluster_method == \"hierarchical\":\n",
    "            clustering_model = AgglomerativeClustering(n_clusters=clusters)\n",
    "            cluster_ids = clustering_model.fit_predict(image_embedding_pca)\n",
    "        elif cluster_method == \"dbscan\":\n",
    "            clustering_model = DBSCAN()\n",
    "            cluster_ids = clustering_model.fit_predict(image_embedding_pca)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported clustering method: {cluster_method}\")\n",
    "\n",
    "        cluster_array_1[clusters].append(cluster_ids)\n",
    "\n",
    "\n",
    "# save cluster_array_1\n",
    "\n",
    "with open(f\"similarities/cub_SAM_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cluster_array_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "for key in cluster_array_1.keys():\n",
    "    local_array = cluster_array_1[key]\n",
    "\n",
    "    nmi_scores = []\n",
    "    model_names_pairs = []\n",
    "    for i in range(len(local_array)):\n",
    "        for j in range(i + 1, len(local_array)):\n",
    "            nmi_score = normalized_mutual_info_score(local_array[i], local_array[j])\n",
    "            model1, model2 = model_names[i], model_names[j]\n",
    "            data.append([key, model1, model2, nmi_score])\n",
    "\n",
    "# Create DataFrame after collecting all data\n",
    "pandas_df = pd.DataFrame(data, columns=[\"Clusters\", \"Model 1\", \"Model 2\", \"NMI_score\"])\n",
    "# Create a pivot table for the heatmap\n",
    "pivot_df = pandas_df.pivot(\n",
    "    index=\"Clusters\", columns=[\"Model 1\", \"Model 2\"], values=\"NMI_score\"\n",
    ")\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(pivot_df, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "plt.title(\"NMI Scores Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the original dataframe as well\n",
    "display(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_technique = \"GDINO\"\n",
    "concept_name = \"partimagenet\"\n",
    "\n",
    "cluster_array_2 = {}\n",
    "\n",
    "for clusters in cluster_array:\n",
    "    cluster_array_2[clusters] = []\n",
    "    model_names = [\"CLIP-ViT-B16\", \"CLIP-ViT-L14\", \"CLIP-RN50\"]\n",
    "\n",
    "    for model_name in model_names:\n",
    "        cbm = CBM(embed_path, dataset, model_name, class_labels_path)\n",
    "        cbm.load_concepts(\n",
    "            segment_path,\n",
    "            segmentation_technique,\n",
    "            concept_name,\n",
    "            selected_image_concepts,\n",
    "            concept_per_class,\n",
    "        )\n",
    "\n",
    "        norm = np.linalg.norm(cbm.image_segments, axis=1, keepdims=True)\n",
    "        image_embedding = cbm.image_segments / norm\n",
    "        # image_embedding = cbm.image_segments\n",
    "\n",
    "        if clusters > image_embedding.shape[0]:\n",
    "            clusters = image_embedding.shape[0]\n",
    "\n",
    "        print(\"Amount of image embeddings: \", image_embedding.shape[0])\n",
    "        print(\"Amount of clusters: \", clusters)\n",
    "\n",
    "        pca = PCA(n_components=100)  # You can adjust the number of components as needed\n",
    "        image_embedding_pca = pca.fit_transform(image_embedding)\n",
    "\n",
    "        cbm.cluster_technique = cluster_method\n",
    "        if cluster_method == \"kmeans\":\n",
    "            clustering_model = KMeans(n_clusters=clusters, random_state=42)\n",
    "            cluster_ids = clustering_model.fit_predict(image_embedding_pca)\n",
    "        elif cluster_method == \"hierarchical\":\n",
    "            clustering_model = AgglomerativeClustering(n_clusters=clusters)\n",
    "            cluster_ids = clustering_model.fit_predict(image_embedding_pca)\n",
    "        elif cluster_method == \"dbscan\":\n",
    "            clustering_model = DBSCAN()\n",
    "            cluster_ids = clustering_model.fit_predict(image_embedding_pca)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported clustering method: {cluster_method}\")\n",
    "\n",
    "        cluster_array_2[clusters].append(cluster_ids)\n",
    "\n",
    "\n",
    "with open(f\"similarities/cub_partimegenet.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cluster_array_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_technique = \"MASKRCNN\"\n",
    "concept_name = None\n",
    "\n",
    "cluster_array_3 = {}\n",
    "\n",
    "for clusters in cluster_array:\n",
    "    cluster_array_3[clusters] = []\n",
    "    model_names = [\"CLIP-ViT-B16\", \"CLIP-ViT-L14\", \"CLIP-RN50\"]\n",
    "\n",
    "    for model_name in model_names:\n",
    "        cbm = CBM(embed_path, dataset, model_name, class_labels_path)\n",
    "        cbm.load_concepts(\n",
    "            segment_path,\n",
    "            segmentation_technique,\n",
    "            concept_name,\n",
    "            selected_image_concepts,\n",
    "            concept_per_class,\n",
    "        )\n",
    "\n",
    "        norm = np.linalg.norm(cbm.image_segments, axis=1, keepdims=True)\n",
    "        image_embedding = cbm.image_segments / norm\n",
    "        # image_embedding = cbm.image_segments\n",
    "\n",
    "        if clusters > image_embedding.shape[0]:\n",
    "            clusters = image_embedding.shape[0]\n",
    "\n",
    "        print(\"Amount of image embeddings: \", image_embedding.shape[0])\n",
    "        print(\"Amount of clusters: \", clusters)\n",
    "\n",
    "        pca = PCA(n_components=100)  # You can adjust the number of components as needed\n",
    "        image_embedding_pca = pca.fit_transform(image_embedding)\n",
    "\n",
    "        cbm.cluster_technique = cluster_method\n",
    "        if cluster_method == \"kmeans\":\n",
    "            clustering_model = KMeans(n_clusters=clusters, random_state=42)\n",
    "            cluster_ids = clustering_model.fit_predict(image_embedding_pca)\n",
    "        elif cluster_method == \"hierarchical\":\n",
    "            clustering_model = AgglomerativeClustering(n_clusters=clusters)\n",
    "            cluster_ids = clustering_model.fit_predict(image_embedding_pca)\n",
    "        elif cluster_method == \"dbscan\":\n",
    "            clustering_model = DBSCAN()\n",
    "            cluster_ids = clustering_model.fit_predict(image_embedding_pca)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported clustering method: {cluster_method}\")\n",
    "\n",
    "        cluster_array_3[clusters].append(cluster_ids)\n",
    "\n",
    "\n",
    "with open(f\"similarities/cub_maskrcnn.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cluster_array_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "nmi_dict = {}\n",
    "\n",
    "for key in cluster_array_1.keys():\n",
    "    local_array = cluster_array_1[key]\n",
    "\n",
    "    nmi_scores = []\n",
    "    model_names_pairs = []\n",
    "    for i in range(len(local_array)):\n",
    "        for j in range(i + 1, len(local_array)):\n",
    "            nmi_score = normalized_mutual_info_score(local_array[i], local_array[j])\n",
    "            model1, model2 = model_names[i], model_names[j]\n",
    "            data.append([key, model1, model2, nmi_score])\n",
    "\n",
    "nmi_dict[\"SAM2\"] = data\n",
    "data = []\n",
    "\n",
    "\n",
    "for key in cluster_array_2.keys():\n",
    "    local_array = cluster_array_2[key]\n",
    "\n",
    "    nmi_scores = []\n",
    "    model_names_pairs = []\n",
    "    for i in range(len(local_array)):\n",
    "        for j in range(i + 1, len(local_array)):\n",
    "            nmi_score = normalized_mutual_info_score(local_array[i], local_array[j])\n",
    "            model1, model2 = model_names[i], model_names[j]\n",
    "            data.append([key, model1, model2, nmi_score])\n",
    "\n",
    "nmi_dict[\"GDINO\"] = data\n",
    "data = []\n",
    "\n",
    "\n",
    "for key in cluster_array_3.keys():\n",
    "    local_array = cluster_array_3[key]\n",
    "\n",
    "    nmi_scores = []\n",
    "    model_names_pairs = []\n",
    "    for i in range(len(local_array)):\n",
    "        for j in range(i + 1, len(local_array)):\n",
    "            nmi_score = normalized_mutual_info_score(local_array[i], local_array[j])\n",
    "            model1, model2 = model_names[i], model_names[j]\n",
    "            data.append([key, model1, model2, nmi_score])\n",
    "\n",
    "nmi_dict[\"MASKRCNN\"] = data\n",
    "\n",
    "# Create and plot heatmaps for all segmentation techniques in nmi_dict\n",
    "fig, axes = plt.subplots(1, 3, figsize=(36, 10))\n",
    "# fig.suptitle(\"NMI Scores Heatmaps for Different Segmentation Techniques\", fontsize=30)\n",
    "\n",
    "for idx, (seg_technique, data) in enumerate(nmi_dict.items()):\n",
    "    # Create DataFrame for the current segmentation technique\n",
    "    pandas_df = pd.DataFrame(\n",
    "        data, columns=[\"Clusters\", \"Model 1\", \"Model 2\", \"NMI_score\"]\n",
    "    )\n",
    "\n",
    "    # Create a pivot table for the heatmap\n",
    "    pivot_df = pandas_df.pivot(\n",
    "        index=\"Clusters\", columns=[\"Model 1\", \"Model 2\"], values=\"NMI_score\"\n",
    "    )\n",
    "\n",
    "    # Create the heatmap with annotations\n",
    "    heatmap = sns.heatmap(\n",
    "        pivot_df, annot=False, cmap=\"YlGnBu\", fmt=\".3f\", ax=axes[idx], cbar=True\n",
    "    )\n",
    "    axes[idx].set_title(f\"{seg_technique} Segmentation\", fontsize=50)\n",
    "    axes[idx].set_ylabel(\"Clusters\", fontsize=35)\n",
    "    axes[idx].set_xlabel(\"Model Pairs\", fontsize=35)\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    axes[idx].set_xticklabels(\n",
    "        axes[idx].get_xticklabels(), rotation=45, ha=\"right\", fontsize=30\n",
    "    )\n",
    "    axes[idx].tick_params(axis=\"y\", labelsize=30)\n",
    "\n",
    "    # Increase text size of color bar and annotations\n",
    "    cbar = heatmap.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=25)\n",
    "    for t in heatmap.texts:\n",
    "        t.set_fontsize(25)\n",
    "\n",
    "    # Adjust figure size to accommodate all labels and annotations\n",
    "    plt.gcf().set_size_inches(50, 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
