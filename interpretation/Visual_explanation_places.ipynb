{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import wandb\n",
    "import re\n",
    "import clip\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# add the parent directory to the path\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "                \n",
    "from utils.dcbm import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Fix -----------------\n",
    "embed_path = \"../data/Embeddings/\"\n",
    "dataset = \"cifar100\"\n",
    "class_labels_path = \"../data/classes/cifar100_classes.txt\"\n",
    "segment_path = \"../data/Segments/\"\n",
    "selected_image_concepts = \"../data/Embeddings/subsets\"\n",
    "raw_path_dataset = # TODO\n",
    "raw_path = #TODO: Define the path to the raw images\n",
    "# ----------------- Hyperparameters -----------------\n",
    "\n",
    "model_name = \"CLIP-ViT-L14\"  # \"CLIP-ViT-L14\", \"CLIP-RN50\", CLIP-ViT-B16\n",
    "\n",
    "segmentation_technique = \"SAM2\"  # GDINO, SAM, SAM2, DETR, MaskRCNN\n",
    "concept_name = None # Define for GDINE [awa, sun, sun-lowthresh, cub...]\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "clusters = 2048\n",
    "cluster_method = \"kmeans\"  # \"hierarchical\", \"kmeans\"\n",
    "centroid_method = \"median\"  # \"mean\", \"median\"\n",
    "\n",
    "concept_per_class = 50  # How many images for each class 5,10,20,50, None\n",
    "\n",
    "one_hot = False\n",
    "epochs = 200\n",
    "lambda_1 = 1e-4\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "\n",
    "crop = False  # True without background\n",
    "\n",
    "use_wandb = False\n",
    "project = \"YOUR_PROJECT_NAME\"  # Define your own project name within wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbm = CBM(\n",
    "    embed_path, dataset, model_name, class_labels_path, device=device\n",
    ")  # Initialize CBM\n",
    "\n",
    "cbm.load_concepts(\n",
    "    segment_path,\n",
    "    segmentation_technique,\n",
    "    concept_name,\n",
    "    selected_image_concepts,\n",
    "    concept_per_class,\n",
    "    crop=crop,\n",
    ")  # Load concepts with predefined segmentation technique and hyperparameters\n",
    "\n",
    "if clusters is not None:  # if clustering is needed\n",
    "    cbm.cluster_image_concepts(cluster_method, clusters, pca = True)\n",
    "else:\n",
    "    cbm.clustered_concepts = cbm.image_segments\n",
    "\n",
    "cbm.centroid_concepts(\n",
    "    centroid_method\n",
    ")  # Calculate centroids of the concepts with given method\n",
    "\n",
    "cbm.preprocess_data(\n",
    "    type_=\"standard\", label_type=one_hot\n",
    ")  # preprocess data for training\n",
    "cbm.train(  # train the model\n",
    "    num_epochs=epochs,\n",
    "    lambda_1=lambda_1,\n",
    "    lr=lr,\n",
    "    device=device,\n",
    "    batch_size=batch_size,\n",
    "    project=project,\n",
    "    to_print=False,\n",
    "    early_stopping_patience=None,\n",
    "    one_hot=one_hot,\n",
    "    use_wandb=use_wandb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbm_model = cbm_model\n",
    "model = CBM_Model(\n",
    "    cbm_model, cbm.clustered_concepts, cbm.preprocess_module, cbm.scaler, device=device\n",
    ")\n",
    "print(\"Predictions: \")\n",
    "print(model.predict_processed(cbm.X_test[:200]))\n",
    "print(\"True Classes: \")\n",
    "print(np.argmax(cbm.y_test[:200], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_image = 2\n",
    "\n",
    "concept_ids, concept_weights = cbm.plot_instance_feature_importance(id_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-L/14\", device=device, jit=False)\n",
    "\n",
    "with open(\"..data/classes/20k.txt\", \"r\") as f:\n",
    "    g20k = f.readlines()\n",
    "names = [i.strip() for i in g20k]\n",
    "tokenized_text = clip.tokenize(names).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(tokenized_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_with_main(image_paths, main_image_path, concept_names, num_secondary=5):\n",
    "    \"\"\"\n",
    "    Display a main image prominently and a series of smaller images in a row layout.\n",
    "\n",
    "    Parameters:\n",
    "    - image_paths (list): List of file paths for the secondary images.\n",
    "    - main_image_path (str): File path of the main image to be displayed larger.\n",
    "    - num_secondary (int): Number of secondary images to display. Default is 5.\n",
    "    \"\"\"\n",
    "    # Remove duplicates from image paths\n",
    "    image_paths = image_paths[:num_secondary]\n",
    "    print(main_image_path)\n",
    "    # print(image_paths)\n",
    "    # Display the main image\n",
    "    main_img = Image.open(main_image_path)\n",
    "    plt.imshow(main_img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Set up the grid: main image in a larger size, followed by smaller images\n",
    "    fig, axes = plt.subplots(1, num_secondary, figsize=(9, 4))\n",
    "\n",
    "    # Display each of the secondary images\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(\"hier\", image_path)\n",
    "        img = cv2.imread(image_path)\n",
    "\n",
    "        # Image data of dtype object cannot be converted to float\n",
    "        try:\n",
    "            axes[i].imshow(img)\n",
    "        except:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            axes[i].imshow(img)\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(concept_names[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_concepts(id_image, cbm, raw_path, raw_path_dataset):\n",
    "\n",
    "    def extract_until_number(strings):\n",
    "        return [process_string(s) for s in strings]\n",
    "\n",
    "    def process_string(s):\n",
    "        match = re.match(r\"^([^0-9]+)\", s)\n",
    "        return match.group(1).strip(\"_\") if match else s\n",
    "\n",
    "    concept_ids, concept_weights = cbm.plot_instance_feature_importance(id_image)\n",
    "\n",
    "    # Get the main image path\n",
    "    print(\"id_image\", id_image)\n",
    "    folder_names = os.listdir(raw_path_dataset)\n",
    "    image_name = list(cbm.data_test_raw.keys())[id_image]\n",
    "    folder_name = image_name.split(\"_Places365\")[0]\n",
    "    image_name = image_name.split(folder_name + \"_\")[1]\n",
    "\n",
    "    image_path_org = os.path.join(raw_path_dataset, folder_name, image_name)\n",
    "    # Find paths to median images based on feature importance (as in your example)\n",
    "    image_median_paths = []\n",
    "    concept_names = []\n",
    "    for idx in concept_ids[:5]:\n",
    "        idx = int(idx)\n",
    "        clustered_concepts = cbm.clustered_concepts_all\n",
    "        data = clustered_concepts[idx]\n",
    "        median_values = np.median(data, axis=0)\n",
    "        distances = np.sum(np.abs(data - median_values), axis=1)\n",
    "        median_index = np.argmin(distances)\n",
    "        clustered_images = cbm.image_segments_names\n",
    "        median_index = np.argsort(distances)\n",
    "        # median_index is numeric which retrieves the embedding and path\n",
    "\n",
    "        # Select up to n images closest to median\n",
    "        n = 20\n",
    "        median_entries = [clustered_images[idx][i] for i in median_index[:n]]\n",
    "        \n",
    "        concept_emb = data[median_index[0]]\n",
    "        concept_emb = torch.tensor(concept_emb).to(device)\n",
    "        sim = torch.nn.functional.cosine_similarity(concept_emb, text_features).cpu().float()\n",
    "\n",
    "        top_index = sim.argsort(descending=True)\n",
    "        concept_names.append(names[top_index[0]])\n",
    "\n",
    "        # Process and add paths for the secondary images\n",
    "        def construct_path(strings):\n",
    "            correct_strings = []\n",
    "            for substring in strings:\n",
    "                folder_name = process_string(substring)\n",
    "                for i in range(len(folder_names)):\n",
    "                    if folder_name.lower() in folder_names[i].lower():\n",
    "                        folder_name = folder_names[i]\n",
    "                        break\n",
    "                substring = os.path.join(raw_path, folder_name, substring)\n",
    "                correct_strings.append(substring)\n",
    "            return correct_strings\n",
    "\n",
    "        image_median_paths.append(construct_path(median_entries)[0])\n",
    "\n",
    "    # Call the function to display images\n",
    "    display_images_with_main(image_median_paths, image_path_org, concept_names)\n",
    "\n",
    "\n",
    "id_image = random.randint(0, len(cbm.data_test_raw))\n",
    "\n",
    "display_concepts(id_image, cbm, raw_path, raw_path_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
